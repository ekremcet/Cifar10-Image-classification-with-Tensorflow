{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h5> Ekrem Ã‡etinkaya S004228 </h5>\n",
    "<hr />\n",
    "<h1> <strong> Cifar-10 Trainer </strong> </h1>\n",
    "<hr />\n",
    "<h3> This notebook will be used for initalizing two networks and training them </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Read data and do preprocessing </h4>\n",
    "<h5> You only need to run this cell once </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_data\n",
    "import numpy as np\n",
    "def normalize(images):\n",
    "    maximum = np.max(images)\n",
    "    minimum = np.min(images)\n",
    "    \n",
    "    return (images - minimum) / (maximum - minimum)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    nx = np.max(labels) + 1\n",
    "    \n",
    "    return np.eye(nx)[labels]\n",
    "read_data.preprocess_and_save_data(normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<h3> CHECKPOINT </h3>\n",
    "<hr />\n",
    "<br>Run from here if you executed above cell before\n",
    "<h4> Read validation images and labels </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "valid_images, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Import tensorflow </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Input variables that will be used in network design </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_input(image_shape):\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], 3], name='image')\n",
    "\n",
    "def label_input(class_count):\n",
    "    return tf.placeholder(tf.float32, [None, class_count], name='label')\n",
    "\n",
    "def dropout_input():\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> CNN variables </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # In case the notebook is run after CHECKPOINT\n",
    "def conv2d(x_tensor, conv_features, conv_filter, conv_strides):\n",
    "    input_depth = x_tensor.get_shape().as_list()[-1]\n",
    "    Weight = tf.Variable(tf.random_normal(shape=[conv_filter[0], conv_filter[1], input_depth, conv_features], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_features))\n",
    "    conv = tf.nn.relu(tf.nn.conv2d(x_tensor, Weight, [1, conv_strides[0], conv_strides[1], 1], 'SAME') + bias)\n",
    "\n",
    "    return conv\n",
    "\n",
    "def max_pool(x_tensor, pool_size, pool_stride):\n",
    "\n",
    "    return tf.nn.max_pool(x_tensor, [1, pool_size[0], pool_size[1], 1], [1, pool_stride[0], pool_stride[1] ,1], padding='SAME')\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "\n",
    "    return tf.reshape(x_tensor, [-1, np.prod(shape[1:])])\n",
    "\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    Weight = tf.Variable(tf.random_normal([shape[-1], num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, Weight), bias))\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    Weight = tf.Variable(tf.random_normal([shape[-1], num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "\n",
    "    return tf.add(tf.matmul(x_tensor, Weight), bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <strong> First CNN </strong> </h3>\n",
    "<ul>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 64 features </li>\n",
    "<li> <strong> Max Pool </strong> 3 x 3 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 64 features </li>\n",
    "<li> <strong> Max Pool </strong> 3 x 3 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 96 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 128 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Flatten layer </strong> </li>\n",
    "<li> <strong> Fully connected layer </strong> 128 </li>\n",
    "<li> <strong> Softmax layer </strong> </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_one(input):\n",
    "    #402K parameters , 71,58% accuracy after 170 iteration\n",
    "    conv1 = conv2d(input, conv_features=64, conv_filter=[3,3], conv_strides=[1,1])\n",
    "    max_pool1 = max_pool(conv1, pool_size=[3,3], pool_stride=[2,2])\n",
    "    tf.nn.dropout(max_pool1, 0.5)\n",
    "    \n",
    "    conv2 = conv2d(max_pool1, 64, [3,3], [1,1])\n",
    "    max_pool2 = max_pool(conv2, [3,3], [2,2])\n",
    "    tf.nn.dropout(max_pool2, 0.5)\n",
    "    \n",
    "    conv3 = conv2d(max_pool2, 96, [3,3], [1,1])\n",
    "    max_pool3 = max_pool(conv3, [2,2], [2,2])\n",
    "    tf.nn.dropout(max_pool3, 0.5)\n",
    "    \n",
    "    conv4 = conv2d(max_pool3, 128, [3,3], [1,1])\n",
    "    max_pool4 = max_pool(conv4, [2,2], [2,2])\n",
    "    tf.nn.dropout(max_pool4, 0.5)\n",
    "    \n",
    "    flattened = flatten(max_pool3)\n",
    "    fc1 = fully_conn(flattened, 128)\n",
    "    \n",
    "    return output(fc1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <strong> Second CNN </strong> </h3>\n",
    "<ul>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 32 features </li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 32 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 25% probability</li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 48 features </li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 48 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 25% probability</li>\n",
    "<li> <strong> Flatten layer </strong> </li>\n",
    "<li> <strong> Fully connected layer </strong> 128 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Softmax layer </strong> </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_two(input):\n",
    "    #440K parameters, 66.09% accuracy after 88 iterations\n",
    "    conv1 = conv2d(input, conv_features=32, conv_filter=[3,3], conv_strides=[1,1])\n",
    "    conv2 = conv2d(conv1, 32, [3,3], [1,1])\n",
    "    max_pool1 = max_pool(conv2, pool_size=[2,2], pool_stride=[2,2])\n",
    "    tf.nn.dropout(max_pool1, 0.25)\n",
    "    \n",
    "    conv3 = conv2d(max_pool1, 48, [3,3], [1,1])\n",
    "    conv4 = conv2d(conv3, 48, [3,3], [1,1])\n",
    "    max_pool2 = max_pool(conv4, [2,2], [2,2])\n",
    "    tf.nn.dropout(max_pool2, 0.25)\n",
    "    \n",
    "    flattened = flatten(max_pool2)\n",
    "    fc1 = fully_conn(flattened, 128)\n",
    "    tf.nn.dropout(fc1, 0.5)\n",
    "    \n",
    "    return output(fc1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Define variables for training </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "images = image_input((32, 32, 3))\n",
    "labels = label_input(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Select which model to train </h4>\n",
    "<hr />\n",
    "<h4> save_model_path will be used while both saving and evaluating the model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits = cnn_model_one(images)\n",
    "#save_model_path =\".\\\\cifar_model_one\"\n",
    "logits = cnn_model_two(images)\n",
    "save_model_path = \".\\\\cifar_model_two\"\n",
    "\n",
    "logits = tf.identity(logits, name='logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Define loss and optimizer </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "# Accuracy\n",
    "correct_result = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_result, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Specify iteration count and batch size </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 200\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Functions to train networks </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_data\n",
    "\n",
    "def train_network(session ,optimizer, image_batch, label_batch):\n",
    "    session.run(optimizer, feed_dict={images: image_batch, labels: label_batch})\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    global valid_features, valid_labels\n",
    "    validation_accuracy = session.run(\n",
    "        accuracy,\n",
    "        feed_dict={\n",
    "            images: valid_images,\n",
    "            labels: valid_labels,\n",
    "        }\n",
    "    )\n",
    "    cost = session.run(\n",
    "        cost,\n",
    "        feed_dict={\n",
    "            images: feature_batch,\n",
    "            labels: label_batch,\n",
    "        }\n",
    "    )\n",
    "    print('Cost = {0} - Validation Accuracy = {1} '.format(cost, validation_accuracy))\n",
    "\n",
    "def train_with_all_batches():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Delete the # below line to restore an existing model and train it\n",
    "        #tf.train.Saver().restore(sess, save_model_path)\n",
    "        for iteration in range(MAX_ITERATIONS):\n",
    "            num_of_batches = 5\n",
    "            for batch_i in range(1, num_of_batches + 1):\n",
    "                print('Iteration {:>2}, CIFAR-10 Batch {}:  '.format(iteration + 1, batch_i), end='')\n",
    "                for batch_images, batch_labels in read_data.load_preprocess_training_batch(batch_i, BATCH_SIZE):\n",
    "                    train_network(sess, optimizer, batch_images, batch_labels)\n",
    "                print_stats(sess, batch_images, batch_labels, cost, accuracy) \n",
    "            saver = tf.train.Saver()\n",
    "            final_save_path = saver.save(sess, save_model_path)\n",
    "        \n",
    "        # Save Model\n",
    "        saver = tf.train.Saver()\n",
    "        final_save_path = saver.save(sess, save_model_path)\n",
    "\n",
    "def train_with_one_batch():    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for iteration in range(MAX_ITERATIONS):\n",
    "            batch_i = 1\n",
    "            print('Iteration {:>2}, CIFAR-10 Batch {}:  '.format(iteration + 1, batch_i), end='')\n",
    "            for batch_images, batch_labels in read_data.load_preprocess_training_batch(batch_i, BATCH_SIZE):\n",
    "                train_network(sess, optimizer, batch_images, batch_labels)\n",
    "            print_stats(sess, batch_images, batch_labels, cost, accuracy)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        final_save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_with_one_batch()\n",
    "train_with_all_batches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}