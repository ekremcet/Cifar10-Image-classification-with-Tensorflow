{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h5> Ekrem Ã‡etinkaya S004228 </h5>\n",
    "<hr />\n",
    "<h1> <strong> Cifar-10 Evaluator </strong> </h1>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Imports </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mcnemar\n",
    "import pickle\n",
    "import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Count number of hidden parameters </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_number_of_parameters():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():  \n",
    "    \tlocal_parameters=1\n",
    "    \tshape = variable.get_shape()  #getting shape of a variable\n",
    "    \tfor i in shape:\n",
    "        \tlocal_parameters*=i.value  #mutiplying dimension values\n",
    "    \ttotal_parameters+=local_parameters\n",
    "    print(\"Total number of hidden parameters: %i\" % total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Choose network to test </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \".\\\\cifar_model_one\"\n",
    "#model_path = \".\\\\cifar_model_two\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Test Function </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "test_image_count = 10000\n",
    "\n",
    "def evaluate_model():\n",
    "    test_images, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(model_path + '.meta')\n",
    "        loader.restore(sess, model_path)\n",
    "\n",
    "        # Get tensors from loaded model\n",
    "        loaded_images = loaded_graph.get_tensor_by_name('image:0')\n",
    "        loaded_labels = loaded_graph.get_tensor_by_name('label:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        \n",
    "        # Print hidden parameter count\n",
    "        display_number_of_parameters()\n",
    "        # Initialize an array of size Test image count\n",
    "        prediction_results = np.zeros(test_image_count)\n",
    "        true_count = 0\n",
    "        current_batch = 0\n",
    "        for train_image_batch, train_label_batch in read_data.batch_images_labels(test_images, test_labels, batch_size):\n",
    "            predictions = sess.run(tf.nn.top_k(tf.nn.softmax(loaded_logits), 1), feed_dict={loaded_images: train_image_batch,\n",
    "                                                                              loaded_labels: train_label_batch})\n",
    "            # This part also will be used while conducting McNemar Test\n",
    "            # For each test image i , value of prediction_results[i] will be :\n",
    "            # 1 if model predicted correct label , 0 if the prediction is wrong\n",
    "            for i in range (0, batch_size - 1):\n",
    "                prediction_label = int(predictions[1][i])\n",
    "                correct_label = int(test_labels[current_batch + i].tolist().index(1))\n",
    "                if(prediction_label == correct_label):\n",
    "                    true_count += 1\n",
    "                    prediction_results[i + current_batch] = 1\n",
    "            current_batch += batch_size\n",
    "        #Save prediction array to further use in McNemar test\n",
    "        mcnemar.save_mcnemar_array(model_path, prediction_results)\n",
    "        accuracy = (true_count / test_image_count) * 100\n",
    "        print('Test Accuracy: {0:.2f} %\\n'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluate the model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\cifar_model_one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hidden parameters: 402858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.58 %\n\n"
     ]
    }
   ],
   "source": [
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> McNemar Test </h3>\n",
    "<hr />\n",
    "<h5> DO NOT RUN THIS BEFORE EVALUATING BOTH MODELS </h5>\n",
    "In order to run McNemar test, both models must be evaluated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Right, Model 2 Wrong : 1556 \nModel 1 Wrong, Model 2 Right : 1007 \nModel 1 Wrong, Model 2 Wrong : 1835 \nModel 1 Right, Model 2 Right : 5602 \n"
     ]
    }
   ],
   "source": [
    "mcnemar.run_mcnemar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}