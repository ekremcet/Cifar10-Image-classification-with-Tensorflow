{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h5> Ekrem Ã‡etinkaya S004228 </h5>\n",
    "<hr />\n",
    "<h1> <strong> Cifar-10 Trainer </strong> </h1>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_data\n",
    "import numpy as np\n",
    "def normalize(images):\n",
    "    maximum = np.max(images)\n",
    "    minimum = np.min(images)\n",
    "    \n",
    "    return (images - minimum) / (maximum - minimum)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    nx = np.max(labels) + 1\n",
    "    \n",
    "    return np.eye(nx)[labels]\n",
    "read_data.preprocess_and_save_data(normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> CHECKPOINT </h3>\n",
    "<br>\n",
    "Run from here if you executed above cell before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "valid_images, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def image_input(image_shape):\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], 3], name='image')\n",
    "\n",
    "def label_input(class_count):\n",
    "    return tf.placeholder(tf.float32, [None, class_count], name='label')\n",
    "\n",
    "def dropout_input():\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <strong> First CNN </strong> </h4>\n",
    "<ul>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 64 features </li>\n",
    "<li> <strong> Max Pool </strong> 3 x 3 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Convolution </strong> 5 x 5 filter, 64 features </li>\n",
    "<li> <strong> Max Pool </strong> 3 x 3 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 64 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Flatten layer </strong> </li>\n",
    "<li> <strong> Fully connected layer </strong> 192 </li>\n",
    "<li> <strong> Softmax layer </strong> </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "def cnn_model_one(x_tensor):\n",
    "    conv1 = model.conv2d(x_tensor, conv_features=64, conv_filter=[3,3], conv_strides=[1,1])\n",
    "    max_pool1 = model.max_pool(conv1, pool_size=[3,3], pool_stride=[2,2])\n",
    "    tf.nn.dropout(max_pool1, 0.5)\n",
    "    \n",
    "    conv2 = model.conv2d(max_pool1, 64, [5,5], [1,1])\n",
    "    max_pool2 = model.max_pool(conv2, [3,3], [2,2])\n",
    "    tf.nn.dropout(max_pool2, 0.5)\n",
    "    \n",
    "    conv3 = model.conv2d(max_pool2, 64, [3,3], [1,1])\n",
    "    max_pool3 = model.max_pool(conv3, [2,2], [2,2])\n",
    "    tf.nn.dropout(max_pool3, 0.5)\n",
    "    \n",
    "    flattened = model.flatten(max_pool3)\n",
    "    fc1 = model.fully_conn(flattened, 192)\n",
    "    \n",
    "    return model.output(fc1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <strong> Second CNN </strong> </h4>\n",
    "<ul>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 32 features </li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 32 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 25% probability</li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 64 features </li>\n",
    "<li> <strong> Convolution </strong> 3 x 3 filter, 64 features </li>\n",
    "<li> <strong> Max Pool </strong> 2 x 2 size, stride 2 </li>\n",
    "<li> <strong> Dropout </strong> 25% probability</li>\n",
    "<li> <strong> Flatten layer </strong> </li>\n",
    "<li> <strong> Fully connected layer </strong> 128 </li>\n",
    "<li> <strong> Dropout </strong> 50% probability</li>\n",
    "<li> <strong> Softmax layer </strong> </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_two(x_tensor):\n",
    "    #870K parameters with flatten 192, %26 with 1 batch 1 iter\n",
    "    conv1 = model.conv2d(x_tensor, conv_features=32, conv_filter=[5,5], conv_strides=[1,1])\n",
    "    conv2 = model.conv2d(conv1, 32, [5,5], [1,1])\n",
    "    max_pool1 = model.max_pool(conv2, pool_size=[2,2], pool_stride=[2,2])\n",
    "    tf.nn.dropout(max_pool1, 0.25)\n",
    "    \n",
    "    conv3 = model.conv2d(max_pool1, 64, [3,3], [1,1])\n",
    "    conv4 = model.conv2d(conv3, 64, [3,3], [1,1])\n",
    "    max_pool2 = model.max_pool(conv4, [2,2], [2,2])\n",
    "    tf.nn.dropout(max_pool2, 0.25)\n",
    "    \n",
    "    flattened = model.flatten(max_pool2)\n",
    "    fc1 = model.fully_conn(flattened, 192)\n",
    "    tf.nn.dropout(fc1, 0.5)\n",
    "    \n",
    "    return model.output(fc1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Define variables for training </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "tf.reset_default_graph()\n",
    "images = image_input((32, 32, 3))\n",
    "labels = label_input(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Select which model to train </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = cnn_model_one(images)\n",
    "# save_model_path =\".\\\\cifar_model_one\"\n",
    "logits = cnn_model_two(images)\n",
    "save_model_path = \".\\\\cifar_model_two\"\n",
    "\n",
    "logits = tf.identity(logits, name='logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Loss, Optimizer and Accuracy define </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "# Accuracy\n",
    "correct_result = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_result, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Specify iteration count and batch size </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Training Functions </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_data\n",
    "\n",
    "def train_network(session ,optimizer, image_batch, label_batch):\n",
    "    session.run(optimizer, feed_dict={images: image_batch, labels: label_batch})\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    global valid_features, valid_labels\n",
    "    validation_accuracy = session.run(\n",
    "        accuracy,\n",
    "        feed_dict={\n",
    "            images: valid_images,\n",
    "            labels: valid_labels,\n",
    "        }\n",
    "    )\n",
    "    cost = session.run(\n",
    "        cost,\n",
    "        feed_dict={\n",
    "            images: feature_batch,\n",
    "            labels: label_batch,\n",
    "        }\n",
    "    )\n",
    "    print('Cost = {0} - Validation Accuracy = {1}'.format(cost, validation_accuracy))\n",
    "\n",
    "def train_with_all_batches():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for iteration in range(MAX_ITERATIONS):\n",
    "            num_of_batches = 5\n",
    "            for batch_i in range(1, num_of_batches + 1):\n",
    "                print('Iteration {:>2}, CIFAR-10 Batch {}:  '.format(iteration + 1, batch_i), end='')\n",
    "                for batch_images, batch_labels in read_data.load_preprocess_training_batch(batch_i, BATCH_SIZE):\n",
    "                    train_network(sess, optimizer, batch_images, batch_labels)\n",
    "                    print_stats(sess, batch_images, batch_labels, cost, accuracy)  \n",
    "    # Save Model\n",
    "        saver = tf.train.Saver()\n",
    "        final_save_path = saver.save(sess, save_model_path)\n",
    "\n",
    "def train_with_one_batch():    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for iteration in range(MAX_ITERATIONS):\n",
    "            batch_i = 1\n",
    "            print('Iteration {:>2}, CIFAR-10 Batch {}:  '.format(iteration + 1, batch_i), end='')\n",
    "            for batch_images, batch_labels in read_data.load_preprocess_training_batch(batch_i, BATCH_SIZE):\n",
    "                train_network(sess, optimizer, batch_images, batch_labels)\n",
    "                print_stats(sess, batch_images, batch_labels, cost, accuracy)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        final_save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1, CIFAR-10 Batch 1:  "
     ]
    }
   ],
   "source": [
    "train_with_one_batch()\n",
    "# train_with_all_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}